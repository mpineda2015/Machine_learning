---
title: "Course Project: Precition Assigment Writeup"
output: html_document
---


# 1. Summary 
In this work we present the final report of the Prediction assignment Writeup
project from Practical Machine Learning course. It was created using Rstudio and published in html format. The main objective of the project is to predict the manner in which six participants did the exercises (This is the "classe" variable in the training set). We applied three popular methods to model the regresions and the best one was applied to the 20 test cases available in the test data and the predictions are submitted in appropriate format to the Course Project Prediction Quiz for automated grading. The methods are: Random Forests (RF), Decision Tree (DT) and Generalized Boosted Model (GBM), as described below. A Confusion Matrix is implemented at the end of each analysis to better visualize the accuracy of the models. We conclude that the Random Forest model is the better model.

# 2. Introduction
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

# 3. Initial data manipulation
In this section we load the data from the URL provided above. We then partinioned the training data set in two and  create a Training set with 70% of the data and a Test set with the rest of the data. We also clean NA, the Near Zero Variance (NZV) variables and the ID variables. 

## 3.1 Data Loading 
In this subsection we load the data.
```{r}
library(caret)
```

```{r}
TrainUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
TestUrl  <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
TrainFile<-"pml-traininig.csv"
TestFile<-"pml-testing.csv"

# download the datasets
if(!file.exists(TrainFile))
{
    download.file(TrainUrl,destfile = TrainFile)
}
training <- read.csv(TrainFile)
if(!file.exists(TestFile))
{
    download.file(TestUrl,destfile = TestFile)
}
testing  <- read.csv(TestFile)
```

## 3.2 Data cleaning 
In this subsection we clean the data.
```{r}
# create a partition using caret with the training dataset on 70,30 ratio
inTrain  <- createDataPartition(training$classe, p=0.7, list=FALSE)

TrainSet <- training[inTrain, ]

TestSet  <- training[-inTrain, ]
dim(TrainSet)
dim(TestSet)
```

```{r}
TrainSet <- TrainSet[, -(1:5)]
TestSet  <- TestSet[, -(1:5)]
dim(TrainSet)
```


```{r}
NZV <- nearZeroVar(TrainSet)
TrainSet <- TrainSet[, -NZV]
TestSet  <- TestSet[, -NZV]
dim(TestSet)
dim(TrainSet)
```

```{r}
AllNA    <- sapply(TrainSet, function(x) mean(is.na(x))) > 0.95
TrainSet <- TrainSet[, AllNA==FALSE]
TestSet  <- TestSet[, AllNA==FALSE]
dim(TestSet)
dim(TrainSet)
```

We get that after cleaning the number of variables is reduced to 53.

# 4. Model Building
In this section we use Random Forests, Decision Tree and Generalized Boosted Model to model the regressions in the Train data set and chose the best one with higher accuracy when applied to the Test dataset to be used for the quiz predictions of this project.

## 4.1 Generalized Boosted Model (GBM)
```{r}
set.seed(301)
controlGBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
modFitGBM  <- train(classe ~ ., data=TrainSet, method = "gbm",
                    trControl = controlGBM, verbose = FALSE)
```

```{r}
predictGBM <- predict(modFitGBM, newdata=TestSet)
confMatGBM <- confusionMatrix(predictGBM, TestSet$classe)
confMatGBM
```

```{r}
 round(confMatGBM$overall['Accuracy'], 4)
```
## 4.2 Decision Tree
```{r}
library(rpart)
library(rpart.plot)
library(rattle)
set.seed(301)
modFitDecTree <- rpart(classe ~ ., data=TrainSet, method="class")
fancyRpartPlot(modFitDecTree)
```

```{r}
predictDecTree <- predict(modFitDecTree, newdata=TestSet, type="class")
confMatDecTree <- confusionMatrix(predictDecTree, TestSet$classe)
confMatDecTree
```

```{r}
 round(confMatDecTree$overall['Accuracy'], 4)
```
## 4.3 Random Forests
```{r}
set.seed(301)
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
modFitRandForest <- train(classe ~ ., data=TrainSet, method="rf",
                          trControl=controlRF)
modFitRandForest$finalModel
```

```{r}
predictRandForest <- predict(modFitRandForest, newdata=TestSet)
confMatRandForest <- confusionMatrix(predictRandForest, TestSet$classe)
confMatRandForest
```

```{r}
round(confMatRandForest$overall['Accuracy'], 4)
```

As calculated above, the accuracy of the 3 regression models are:
RF: 0.998 
DT: 0.746 
GBM: 0.9842

Therefore, the RF model will be used to predict the 20 quiz results with the testing dataset.

# 4. Appliying the Random forest model to the test data
In this section we apply the RF model method to the testing data as below
```{r}
predictTEST <- predict(modFitRandForest, newdata=testing)
predictTEST
```

